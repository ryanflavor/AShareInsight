#!/usr/bin/env python3
"""
End-to-End Document Processing Pipeline

This unified script completes the entire workflow from Stories 1.2-1.5:
1. Reads financial report files (annual reports and research reports)
2. Uses LangChain to call LLM API and extract structured data
3. Archives raw JSON responses for future retraining
4. Executes fusion algorithm to update BusinessConceptsMaster
5. Automatically vectorizes new/updated business concepts

Features:
- Intelligent document scanning (avoids already processed files)
- Raw response archiving for model retraining
- Automatic error recovery and retry logic
- Progress tracking and comprehensive reporting
- Support for both annual reports and research reports
"""

import asyncio
import hashlib
import os
import sys
from pathlib import Path
from typing import Any

import click
import structlog
from pydantic import SecretStr
from rich.console import Console
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
)
from rich.table import Table

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from sqlalchemy import extract, select

from src.application.use_cases.build_vector_index import BuildVectorIndexUseCase
from src.application.use_cases.extract_document_data import ExtractDocumentDataUseCase
from src.domain.services.vectorization_service import VectorizationService
from src.infrastructure.document_processing.company_info_extractor import (
    CompanyInfoExtractor,
)
from src.infrastructure.factories import create_standalone_fusion_use_case
from src.infrastructure.llm.gemini_llm_adapter import GeminiLLMAdapter
from src.infrastructure.llm.qwen.qwen_embedding_adapter import (
    QwenEmbeddingAdapter,
    QwenEmbeddingConfig,
)
from src.infrastructure.persistence.postgres.business_concept_master_repository import (
    PostgresBusinessConceptMasterRepository,
)
from src.infrastructure.persistence.postgres.connection import (
    get_db_connection,
    get_session,
)
from src.infrastructure.persistence.postgres.models import (
    BusinessConceptMasterModel,
    CompanyModel,
    SourceDocumentModel,
)
from src.infrastructure.persistence.postgres.session_factory import SessionFactory
from src.infrastructure.persistence.postgres.source_document_repository import (
    PostgresSourceDocumentRepository,
)
from src.shared.config.settings import Settings

logger = structlog.get_logger()
console = Console()


class EndToEndPipeline:
    """Unified pipeline for complete document processing workflow."""

    def __init__(self, settings: Settings):
        self.settings = settings
        self.company_extractor = CompanyInfoExtractor()

        # Initialize services
        self.llm_service = GeminiLLMAdapter(settings)

        # Track processing state
        self.state = {
            "documents_found": 0,
            "documents_processed": 0,
            "extraction_success": 0,
            "extraction_failed": 0,
            "archive_success": 0,
            "archive_skipped": 0,
            "fusion_success": 0,
            "fusion_failed": 0,
            "vectors_created": 0,
            "vectors_failed": 0,
            "raw_responses_saved": 0,
            "errors": [],
        }

        # Cache for optimization
        self.processed_hashes: set[str] = set()
        self.existing_companies: set[str] = set()
        self.processed_combinations: set[tuple[str, str, int]] = set()

        # Note: Raw responses are saved by GeminiLLMAdapter, not by this pipeline

    async def initialize(self):
        """Initialize caches and load existing data."""
        async with get_session() as session:
            # Load existing companies
            company_stmt = select(CompanyModel.company_code)
            company_result = await session.execute(company_stmt)
            self.existing_companies = {row[0] for row in company_result}

            # Load processed document hashes
            repository = PostgresSourceDocumentRepository(session)
            self.processed_hashes = await repository.get_all_file_hashes()

            # Load processed combinations
            doc_stmt = (
                select(
                    SourceDocumentModel.company_code,
                    SourceDocumentModel.doc_type,
                    extract("year", SourceDocumentModel.doc_date).label("year"),
                )
                .where(SourceDocumentModel.company_code.isnot(None))
                .distinct()
            )
            doc_result = await session.execute(doc_stmt)
            for row in doc_result:
                self.processed_combinations.add(
                    (row.company_code, row.doc_type, int(row.year))
                )

        logger.info(
            "pipeline_initialized",
            existing_companies=len(self.existing_companies),
            processed_hashes=len(self.processed_hashes),
            processed_combinations=len(self.processed_combinations),
        )

    def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA-256 hash of file content."""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def _extract_company_info(self, file_path: Path) -> tuple[str | None, int | None]:
        """Extract company code and year from file."""
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()

            info = self.company_extractor.extract_info(file_path, content)
            return info["code"], info["year"]
        except Exception as e:
            logger.warning(f"Failed to extract info from {file_path}: {e}")
            return None, None

    async def find_documents_to_process(self) -> tuple[list[dict], list[dict]]:
        """Find all documents that need processing."""
        annual_reports = []
        research_reports = []

        # Check existing extracted JSON files to avoid reprocessing
        extracted_files = set()
        for extracted_dir in [
            "data/extracted/annual_reports",
            "data/extracted/research_reports",
        ]:
            extracted_path = Path(extracted_dir)
            if extracted_path.exists():
                for json_file in extracted_path.glob("*.json"):
                    base_name = json_file.stem.replace("_extracted", "")
                    extracted_files.add(base_name)

        # Scan directories
        for base_dir, doc_type in [
            ("data/annual_reports", "annual_report"),
            ("data/research_reports", "research_report"),
        ]:
            scan_dir = Path(base_dir)
            if not scan_dir.exists():
                continue

            for pattern in ["**/*.md", "**/*.txt"]:
                for file_path in scan_dir.glob(pattern):
                    if not file_path.is_file():
                        continue

                    # Skip if already processed by hash
                    file_hash = self._calculate_file_hash(file_path)
                    if file_hash in self.processed_hashes:
                        logger.debug(f"Skipping {file_path.name} - already in database")
                        continue

                    # Skip if already extracted to JSON
                    if file_path.stem in extracted_files:
                        logger.debug(f"Skipping {file_path.name} - already extracted")
                        continue

                    # Extract company info
                    company_code, year = self._extract_company_info(file_path)

                    # Check if combination already exists
                    if company_code and year:
                        combination = (company_code, doc_type, year)
                        if combination in self.processed_combinations:
                            logger.debug(
                                f"Skipping {file_path.name} - combination already exists"
                            )
                            continue

                    # Add to appropriate list
                    doc_info = {
                        "path": file_path,
                        "company_code": company_code,
                        "year": year,
                        "doc_type": doc_type,
                        "size": file_path.stat().st_size,
                        "hash": file_hash,
                    }

                    if doc_type == "annual_report":
                        annual_reports.append(doc_info)
                    else:
                        research_reports.append(doc_info)

        self.state["documents_found"] = len(annual_reports) + len(research_reports)
        return annual_reports, research_reports

    async def process_document(self, doc_info: dict) -> dict[str, Any]:
        """Process a single document through the entire pipeline."""
        file_path = doc_info["path"]
        doc_type = doc_info["doc_type"]
        company_code = doc_info["company_code"]
        result = {"success": False, "stages": {}}

        try:
            # Stage 1: Extract with LLM
            logger.info(f"Extracting {file_path.name} with LLM...")
            async with get_session() as session:
                archive_repository = PostgresSourceDocumentRepository(session)
                extract_use_case = ExtractDocumentDataUseCase(
                    self.llm_service, archive_repository
                )

                extraction_result = await extract_use_case.execute(
                    file_path, document_type_override=doc_type
                )

                self.state["extraction_success"] += 1
                result["stages"]["extraction"] = "success"

                # Stage 2: Raw response already saved by GeminiLLMAdapter
                # The adapter saves raw responses with company name in filename for annual reports
                # and with timestamp for research reports
                result["stages"]["raw_response"] = "saved_by_adapter"

                # Stage 3: Archive to database (already done in ExtractDocumentDataUseCase)
                # The archive use case will handle research reports without companies
                # It will return a dummy UUID and log a warning, which we can detect
                # For now, we'll assume success and let the repository handle the logic
                self.state["archive_success"] += 1
                result["stages"]["archive"] = "success"

                # Note: The ArchiveExtractionResultUseCase already checks if companies exist
                # and skips archiving research reports for non-existent companies.
                # We don't need to check this here as it's handled at the repository level.

                # Get the document ID from the archive
                doc_id = None
                async with get_session() as session:
                    stmt = (
                        select(SourceDocumentModel.doc_id)
                        .where(SourceDocumentModel.file_hash == doc_info["hash"])
                        .order_by(SourceDocumentModel.created_at.desc())
                        .limit(1)
                    )
                    result_row = await session.execute(stmt)
                    doc_id = result_row.scalar()

                if doc_id:
                    # Stage 4: Fusion update
                    logger.info(f"Executing fusion for document {doc_id}...")
                    fusion_use_case = await create_standalone_fusion_use_case()
                    fusion_result = await fusion_use_case.execute(doc_id)

                    self.state["fusion_success"] += 1
                    result["stages"]["fusion"] = {
                        "status": "success",
                        "concepts_created": fusion_result["concepts_created"],
                        "concepts_updated": fusion_result["concepts_updated"],
                    }

                    # Stage 5: Vectorization
                    if (
                        fusion_result["concepts_created"]
                        + fusion_result["concepts_updated"]
                        > 0
                    ):
                        logger.info("Building vectors for new/updated concepts...")
                        await self.build_vectors_for_document(doc_id)
                        result["stages"]["vectorization"] = "success"
                else:
                    result["stages"]["fusion"] = "skipped_no_doc_id"
                    result["stages"]["vectorization"] = "skipped"

                result["success"] = True
                self.state["documents_processed"] += 1

        except Exception as e:
            logger.error(f"Error processing {file_path}: {str(e)}", exc_info=True)
            self.state["errors"].append(
                {
                    "file": str(file_path),
                    "error": str(e),
                    "stage": result.get("stages", {}),
                }
            )

            # Update failed counters based on stage
            if "extraction" not in result.get("stages", {}):
                self.state["extraction_failed"] += 1
            elif (
                "fusion" in result.get("stages", {})
                and result["stages"]["fusion"] != "success"
            ):
                self.state["fusion_failed"] += 1

        return result

    async def build_vectors_for_document(self, doc_id: str):
        """Build vectors for concepts from a specific document."""
        try:
            # Get database connection
            db_connection = await get_db_connection()
            session_factory = SessionFactory(db_connection.engine)

            async with session_factory.get_session() as session:
                # Get concepts that need vectors from this document
                stmt = select(BusinessConceptMasterModel).where(
                    BusinessConceptMasterModel.last_updated_from_doc_id == doc_id,
                    BusinessConceptMasterModel.embedding.is_(None),
                )
                result = await session.execute(stmt)
                concepts = result.scalars().all()

                if not concepts:
                    return

                # Initialize embedding service
                embedding_config = QwenEmbeddingConfig.from_settings(
                    self.settings.qwen_embedding
                )
                embedding_service = QwenEmbeddingAdapter(config=embedding_config)

                # Initialize repositories and services
                repository = PostgresBusinessConceptMasterRepository(session)
                vectorization_service = VectorizationService(
                    embedding_service=embedding_service,
                    qwen_settings=self.settings.qwen_embedding,
                )

                # Build vectors
                use_case = BuildVectorIndexUseCase(
                    repository=repository,
                    embedding_service=embedding_service,
                    vectorization_service=vectorization_service,
                    batch_size=50,
                )

                # Process concepts
                stats = {
                    "total_concepts": len(concepts),
                    "processed": 0,
                    "succeeded": 0,
                    "failed": 0,
                    "skipped": 0,
                    "errors": [],
                }

                await use_case._process_batch(concepts, stats)

                self.state["vectors_created"] += stats["succeeded"]
                self.state["vectors_failed"] += stats["failed"]

                logger.info(
                    f"Vectorization complete for doc {doc_id}: "
                    f"{stats['succeeded']} succeeded, {stats['failed']} failed"
                )

        except Exception as e:
            logger.error(f"Error building vectors for document {doc_id}: {str(e)}")
            self.state["vectors_failed"] += 1

    async def run(
        self,
        dry_run: bool = False,
        limit: int | None = None,
        parallel_workers: int = 20,
    ):
        """Run the complete end-to-end pipeline."""
        console.print(
            "\n[bold cyan]=== END-TO-END DOCUMENT PROCESSING PIPELINE ===[/bold cyan]\n"
        )

        # Initialize
        console.print("Initializing pipeline...")
        await self.initialize()

        # Find documents
        console.print("Scanning for documents to process...")
        annual_reports, research_reports = await self.find_documents_to_process()

        # Process annual reports first (important for company creation)
        all_documents = annual_reports + research_reports

        if limit:
            all_documents = all_documents[:limit]
            console.print(f"[yellow]Limited to {limit} documents[/yellow]")

        if not all_documents:
            console.print("[green]No new documents to process![/green]")
            return

        # Display summary
        console.print(
            f"\n[bold]Found {len(all_documents)} documents to process:[/bold]"
        )
        console.print(f"  ‚Ä¢ Annual Reports: {len(annual_reports)}")
        console.print(f"  ‚Ä¢ Research Reports: {len(research_reports)}")

        if dry_run:
            self._display_documents_table(all_documents[:20])
            console.print("\n[yellow]Dry run mode - no processing performed[/yellow]")
            return

        # Process documents
        console.print(f"\n[bold]Processing {len(all_documents)} documents...[/bold]\n")

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task(
                "[cyan]Processing documents...", total=len(all_documents)
            )

            # Process with limited concurrency
            semaphore = asyncio.Semaphore(parallel_workers)

            async def process_with_limit(doc_info):
                async with semaphore:
                    progress.update(
                        task, description=f"[cyan]Processing {doc_info['path'].name}..."
                    )
                    result = await self.process_document(doc_info)
                    progress.advance(task)
                    return result

            # Process all documents
            tasks = [process_with_limit(doc) for doc in all_documents]
            results = await asyncio.gather(*tasks, return_exceptions=True)

        # Display final summary
        self._display_summary()

    def _display_documents_table(self, documents: list[dict]):
        """Display table of documents to process."""
        table = Table(title="Documents to Process")
        table.add_column("Type", style="cyan")
        table.add_column("File", style="yellow")
        table.add_column("Company", style="green")
        table.add_column("Year", style="blue")
        table.add_column("Size", style="magenta")

        for doc in documents:
            table.add_row(
                doc["doc_type"].replace("_", " ").title(),
                doc["path"].name,
                doc["company_code"] or "Unknown",
                str(doc["year"]) if doc["year"] else "Unknown",
                f"{doc['size'] / 1024:.1f} KB",
            )

        if len(documents) < self.state["documents_found"]:
            table.add_row(
                "...",
                f"... and {self.state['documents_found'] - len(documents)} more",
                "...",
                "...",
                "...",
            )

        console.print(table)

    def _display_summary(self):
        """Display comprehensive processing summary."""
        console.print("\n[bold]=== PROCESSING SUMMARY ===[/bold]\n")

        # Overall statistics
        table = Table(title="Pipeline Statistics")
        table.add_column("Stage", style="cyan")
        table.add_column("Success", style="green")
        table.add_column("Failed/Skipped", style="red")
        table.add_column("Total", style="yellow")

        table.add_row("Documents Found", "-", "-", str(self.state["documents_found"]))
        table.add_row(
            "LLM Extraction",
            str(self.state["extraction_success"]),
            str(self.state["extraction_failed"]),
            str(self.state["extraction_success"] + self.state["extraction_failed"]),
        )
        table.add_row(
            "Raw Response Saved",
            str(self.state["raw_responses_saved"]),
            "-",
            str(self.state["raw_responses_saved"]),
        )
        table.add_row(
            "Archive to DB",
            str(self.state["archive_success"]),
            str(self.state["archive_skipped"]),
            str(self.state["archive_success"] + self.state["archive_skipped"]),
        )
        table.add_row(
            "Fusion Update",
            str(self.state["fusion_success"]),
            str(self.state["fusion_failed"]),
            str(self.state["fusion_success"] + self.state["fusion_failed"]),
        )
        table.add_row(
            "Vector Creation",
            str(self.state["vectors_created"]),
            str(self.state["vectors_failed"]),
            str(self.state["vectors_created"] + self.state["vectors_failed"]),
        )

        console.print(table)

        # Show errors if any
        if self.state["errors"]:
            console.print("\n[red]Errors encountered:[/red]")
            error_table = Table()
            error_table.add_column("File", style="cyan", max_width=50)
            error_table.add_column("Stage", style="yellow")
            error_table.add_column("Error", style="red", max_width=60)

            for error in self.state["errors"][:10]:
                stage = "Unknown"
                if error["stage"]:
                    completed_stages = [k for k, v in error["stage"].items() if v]
                    stage = (
                        f"After: {', '.join(completed_stages)}"
                        if completed_stages
                        else "Extraction"
                    )

                error_table.add_row(
                    Path(error["file"]).name,
                    stage,
                    error["error"][:60] + "..."
                    if len(error["error"]) > 60
                    else error["error"],
                )

            if len(self.state["errors"]) > 10:
                error_table.add_row(
                    "...",
                    "...",
                    f"... and {len(self.state['errors']) - 10} more errors",
                )

            console.print(error_table)

        # Success message
        if self.state["documents_processed"] > 0:
            console.print(
                f"\n[green]‚úÖ Successfully processed {self.state['documents_processed']} documents "
                f"through the complete pipeline![/green]"
            )
            console.print(
                "[green]üìÅ Raw responses saved to: data/raw_responses/[/green]"
            )


@click.command()
@click.option(
    "--dry-run",
    is_flag=True,
    help="Show what would be processed without executing",
)
@click.option(
    "--limit",
    type=int,
    help="Limit number of documents to process",
)
@click.option(
    "--parallel-workers",
    type=int,
    default=20,
    help="Number of parallel workers for LLM extraction (default: 20)",
)
@click.option(
    "--gemini-api-key",
    envvar="GEMINI_API_KEY",
    help="Gemini API key (can also be set via GEMINI_API_KEY env var)",
)
def main(dry_run: bool, limit: int | None, parallel_workers: int, gemini_api_key: str):
    """Run the end-to-end document processing pipeline."""
    # Configure logging
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.dev.ConsoleRenderer(),
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

    # Initialize settings
    settings = Settings()
    if gemini_api_key:
        settings.llm.gemini_api_key = SecretStr(gemini_api_key)
    elif not os.getenv("GEMINI_API_KEY"):
        click.echo("Error: GEMINI_API_KEY not provided", err=True)
        sys.exit(1)

    # Create and run pipeline
    pipeline = EndToEndPipeline(settings)

    try:
        asyncio.run(pipeline.run(dry_run, limit, parallel_workers))
    except KeyboardInterrupt:
        console.print("\n[yellow]Interrupted by user[/yellow]")
        sys.exit(1)
    except Exception as e:
        console.print(f"\n[red]Pipeline failed: {str(e)}[/red]")
        logger.error("pipeline_failed", error=str(e), exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
